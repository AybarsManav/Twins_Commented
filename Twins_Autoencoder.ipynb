{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Twins_Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from copy import deepcopy\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "from itertools import repeat\n",
        "import collections.abc"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-08-05T14:58:51.461186Z",
          "iopub.execute_input": "2022-08-05T14:58:51.462167Z",
          "iopub.status.idle": "2022-08-05T14:58:51.468107Z",
          "shell.execute_reply.started": "2022-08-05T14:58:51.462119Z",
          "shell.execute_reply": "2022-08-05T14:58:51.466862Z"
        },
        "trusted": true,
        "id": "OUn97ZAnh6st"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prequisite classes\n"
      ],
      "metadata": {
        "id": "Fnnqzjp5h6sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, bias=True, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        bias = to_2tuple(bias)\n",
        "        drop_probs = to_2tuple(drop)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[0])\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop_probs[0])\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])\n",
        "        self.drop2 = nn.Dropout(drop_probs[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
        "            return x\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse\n",
        "\n",
        "to_2tuple = _ntuple(2)\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.scale_by_keep = scale_by_keep\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'drop_prob={round(self.drop_prob,3):0.3f}'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T14:53:04.377972Z",
          "iopub.execute_input": "2022-08-05T14:53:04.378362Z",
          "iopub.status.idle": "2022-08-05T14:53:04.390406Z",
          "shell.execute_reply.started": "2022-08-05T14:53:04.378331Z",
          "shell.execute_reply": "2022-08-05T14:53:04.389648Z"
        },
        "trusted": true,
        "id": "drqPgEtMh6sy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Size_ = Tuple[int, int]\n",
        "\n",
        "class LocallyGroupedAttn(nn.Module):\n",
        "    \"\"\" LSA: self attention within a group\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0., ws=1):\n",
        "        assert ws != 1\n",
        "        super(LocallyGroupedAttn, self).__init__()\n",
        "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.ws = ws\n",
        "\n",
        "    def forward(self, x, size: Size_):\n",
        "        # There are two implementations for this function, zero padding or mask. We don't observe obvious difference for\n",
        "        # both. You can choose any one, we recommend forward_padding because it's neat. However,\n",
        "        # the masking implementation is more reasonable and accurate.\n",
        "        B, N, C = x.shape\n",
        "        # x: B,H*W,C\n",
        "        H, W = size\n",
        "        x = x.view(B, H, W, C) # x: B, H ,W , C\n",
        "        pad_l = pad_t = 0\n",
        "        pad_r = (self.ws - W % self.ws) % self.ws\n",
        "        pad_b = (self.ws - H % self.ws) % self.ws\n",
        "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b)) # padds last 3 dimensions, channel by 0,0, height by pad_l, pad_r, width by pad_t, pad_b\n",
        "        # Hp = H + pad_r, Wp = W + pad_b \n",
        "        _, Hp, Wp, _ = x.shape\n",
        "        _h, _w = Hp // self.ws, Wp // self.ws #_h and _w will be the number of the padded smaller windows horizontally and vertically on 2D\n",
        "        x = x.reshape(B, _h, self.ws, _w, self.ws, C).transpose(2, 3) #B,_h,_w,patches in vertical windows, patches in horizontal windows, channels for each patch\n",
        "        #qkv: each of q k v, B, _h*_w, heads, num_windows,  dim_head\n",
        "        qkv = self.qkv(x).reshape(\n",
        "            B, _h * _w, self.ws * self.ws, 3, self.num_heads, C // self.num_heads).permute(3, 0, 1, 4, 2, 5)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # q,k,v: B, _h*_w, heads, num_patches, dim_head\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale # attn: B, _h * _w, heads, num_patches,num_patches -- dot product between each patch in each window\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        attn = (attn @ v).transpose(2, 3).reshape(B, _h, _w, self.ws, self.ws, C) # Attention is done, we get attn: B, _h, _w,  num_patches , num_patches, channels\n",
        "        x = attn.transpose(2, 3).reshape(B, _h * self.ws, _w * self.ws, C) # getting back the whole image again with x: B, padded height, padded width, channels\n",
        "        if pad_r > 0 or pad_b > 0: # If padded somehow, remove the elements in the places of paddings\n",
        "            x = x[:, :H, :W, :].contiguous() # x: B, H, W, C\n",
        "        x = x.reshape(B, N, C) # x: B, H*W, C\n",
        "        x = self.proj(x) #projection of the x tensor to dim channels I DID NOT NDERSTAND WHY EXACTLY\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "    \n",
        "class GlobalSubSampleAttn(nn.Module):\n",
        "    \"\"\" GSA: using a  key to summarize the information for a group to be efficient.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(dim, dim, bias=True)\n",
        "        self.kv = nn.Linear(dim, dim * 2, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.sr_ratio = sr_ratio\n",
        "        if sr_ratio > 1:\n",
        "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "        else:\n",
        "            self.sr = None\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x, size: Size_):\n",
        "        B, N, C = x.shape\n",
        "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3) # q: B,num_heads, H*W, head_dim\n",
        "        # We get the queries without supsampling x\n",
        "        if self.sr is not None:\n",
        "            x = x.permute(0, 2, 1).reshape(B, C, *size) # B, C, H, W\n",
        "            x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1) # x: B, C, H/sr, W/sr -> B, C, H/sr*W/sr -- Subsampling\n",
        "            x = self.norm(x)\n",
        "        # After subsampling we get the keys and the values\n",
        "        kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # k and v, B, num_heads, H/sr * W/sr, head_dim\n",
        "        k, v = kv[0], kv[1] # k and v : B, num_heads, H/sr * W/sr, head_dim\n",
        "        # q: B,num_heads, H*W, head_dim\n",
        "        # k.transpose(-2,-1) : B, num_heads, head_dim, H/sr * W/sr\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale # attn: B, num_heads, H*W, H/sr * W/sr\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # attn: B, num_heads, H*W, H/sr * W/sr\n",
        "        # v : B, num_heads, H/sr * W/sr, head_dim\n",
        "        # (attn @ v) : B, num_heads, H*W, head_dim\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C) # x: B, H*W, C == head_dim * num_heads\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0.,\n",
        "            act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, ws=None):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        if ws is None:\n",
        "            self.attn = Attention(dim, num_heads, False, None, attn_drop, drop)\n",
        "        elif ws == 1:\n",
        "            self.attn = GlobalSubSampleAttn(dim, num_heads, attn_drop, drop, sr_ratio)\n",
        "        else:\n",
        "            self.attn = LocallyGroupedAttn(dim, num_heads, attn_drop, drop, ws)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, size: Size_):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x), size))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x # x: B, H*W, C\n",
        "    \n",
        "class PosConv(nn.Module):\n",
        "    # PEG  from https://arxiv.org/abs/2102.10882\n",
        "    def __init__(self, in_chans, embed_dim=768, stride=1):\n",
        "      # in_chans: Image channels after patch embedding, embed_dim: channels after projection, in_channels must be divisible by embed_dim\n",
        "        super(PosConv, self).__init__()\n",
        "        self.proj = nn.Sequential(nn.Conv2d(in_chans, embed_dim, 3, stride, 1, bias=True, groups=embed_dim), )\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, size: Size_):\n",
        "        B, N, C = x.shape\n",
        "        cnn_feat_token = x.transpose(1, 2).view(B, C, *size) # cnn_feat_token: B, C, H,W\n",
        "        x = self.proj(cnn_feat_token) # x: B, embed_dim, H,W -- due to stride and size 3 kernel\n",
        "        if self.stride == 1: # Apparently for this to work class PatchEmbed should output an embedding with the same dimensions as PosCov\n",
        "            x += cnn_feat_token\n",
        "        x = x.flatten(2).transpose(1, 2) # x: B, H*W, embed_dim\n",
        "        return x\n",
        "    \n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size) # img_size = (img_size, img_size)\n",
        "        patch_size = to_2tuple(patch_size) # patch_size = (patch_size, patch_size)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
        "            f\"img_size {img_size} should be divided by patch_size {patch_size}.\"\n",
        "        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1] # H,W: number of patches in vertical and horizontal directions respectively as a 2D map -- Considering each patch as a pixel, H and W makes sense\n",
        "        self.num_patches = self.H * self.W\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) # Nonoverlapping convolutions to embed each patch\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x) -> Tuple[torch.Tensor, Size_]:\n",
        "        B, C, H, W = x.shape # Apparently first input is an image with dimensions: B, C, orig_H, orig_W\n",
        "\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # x: B, H'*W', embed_dim -- H' and W' are the new image where every pixel is actually a patch with 768 dimensions\n",
        "        x = self.norm(x)\n",
        "        out_size = (H // self.patch_size[0], W // self.patch_size[1])\n",
        "\n",
        "        return x, out_size # Return both the patch embedded x and its new size\n",
        "    \n",
        "class Twins(nn.Module):\n",
        "    \"\"\" Twins Vision Transfomer (Revisiting Spatial Attention)\n",
        "    Adapted from PVT (PyramidVisionTransformer) class at https://github.com/whai362/PVT.git\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self, img_size=32, patch_size=4, in_chans=3, num_classes=1000, global_pool='avg',\n",
        "            embed_dims=(64, 128, 256, 512), num_heads=(1, 2, 4, 8), mlp_ratios=(4, 4, 4, 4), depths=(3, 4, 6, 3),\n",
        "            sr_ratios=(8, 4, 2, 1), wss=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
        "            norm_layer=partial(nn.LayerNorm, eps=1e-6), block_cls=Block):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes # For image classification\n",
        "        self.global_pool = global_pool\n",
        "        self.depths = depths\n",
        "        self.embed_dims = embed_dims\n",
        "        self.num_features = embed_dims[-1]\n",
        "        self.grad_checkpointing = False\n",
        "\n",
        "        img_size = to_2tuple(img_size)\n",
        "        prev_chs = in_chans # original image channels\n",
        "        self.patch_embeds = nn.ModuleList()\n",
        "        self.pos_drops = nn.ModuleList() # For dropout\n",
        "        for i in range(len(depths)):\n",
        "            self.patch_embeds.append(PatchEmbed(img_size, patch_size, prev_chs, embed_dims[i])) # Initilialize the PatchEmbed modules with different embed_dim for each layer\n",
        "            self.pos_drops.append(nn.Dropout(p=drop_rate))\n",
        "            prev_chs = embed_dims[i] # Update the prev_chs so we can use PatchEmbed modules\n",
        "            img_size = tuple(t // patch_size for t in img_size) # Gradual patching, declaring the new img sizes while passing through each layer\n",
        "            patch_size = 2\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "        cur = 0\n",
        "        for k in range(len(depths)):\n",
        "            _block = nn.ModuleList([block_cls(\n",
        "                dim=embed_dims[k], num_heads=num_heads[k], mlp_ratio=mlp_ratios[k], drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[k],\n",
        "                ws=1 if wss is None or i % 2 == 1 else wss[k]) for i in range(depths[k])]) # In a layer with depth 3 for example the i=0 th layer will do LSA and i = 1 will do GloalSubsampled\n",
        "            self.blocks.append(_block)\n",
        "            cur += depths[k]\n",
        "\n",
        "        self.pos_block = nn.ModuleList([PosConv(embed_dim, embed_dim) for embed_dim in embed_dims])\n",
        "\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        for i, (embed, drop, blocks, pos_blk) in enumerate(\n",
        "                zip(self.patch_embeds, self.pos_drops, self.blocks, self.pos_block)):\n",
        "            x, size = embed(x) # First create and embed the patches\n",
        "            x = drop(x)\n",
        "            for j, blk in enumerate(blocks): # Pass thorugh attention modules but after the first layer add PEG \n",
        "                x = blk(x, size) \n",
        "                if j == 0:\n",
        "                    x = pos_blk(x, size)  # PEG here\n",
        "            if i < len(self.depths) - 1: # Reshape the x to B, C, H, W unless it is the final output tensor, the final x is x: B, last_H*last_W, last_embedding_dim\n",
        "                x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        x = self.norm(x)\n",
        "        x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        return x\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T15:00:10.627403Z",
          "iopub.execute_input": "2022-08-05T15:00:10.627913Z",
          "iopub.status.idle": "2022-08-05T15:00:10.695204Z",
          "shell.execute_reply.started": "2022-08-05T15:00:10.627861Z",
          "shell.execute_reply": "2022-08-05T15:00:10.693787Z"
        },
        "trusted": true,
        "id": "NgTgRY0Hh6sz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proj = nn.ConvTranspose2d(512,512,3,1,1,bias=True)\n",
        "test = torch.rand(5,512,32,32)\n",
        "oup = proj(test)\n",
        "print(oup.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOP863HSs4b1",
        "outputId": "f17647be-4593-47ec-aee9-3ce888a44762"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 512, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Inv_PosConv(nn.Module):\n",
        "  def __init__(self,in_chans,embed_dim,stride=1):\n",
        "    super(Inv_PosConv,self).__init__()\n",
        "    self.proj = nn.Sequential(nn.ConvTranspose2d(in_chans, embed_dim, 3, stride, 1, bias=True, groups=embed_dim),)\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self,x,size:Size_):\n",
        "    B,N,C = x.shape\n",
        "    # print(x.shape)\n",
        "    cnn_feat_token = x.transpose(1, 2).view(B, C, *size) # cnn_feat_token: B, C, H,W\n",
        "    # print(cnn_feat_token.shape)\n",
        "    x = self.proj(cnn_feat_token) # x: B, embed_dim, H,W -- due to stride and size 3 kernel\n",
        "    # print(x.shape)\n",
        "    if self.stride == 1: # Apparently for this to work class PatchEmbed should output an embedding with the same dimensions as PosCov\n",
        "        x += cnn_feat_token\n",
        "    x = x.flatten(2).transpose(1, 2) # x: B, H*W, embed_dim\n",
        "    return x\n",
        "\n",
        "class Inv_PatchEmbed(nn.Module):\n",
        "  def __init__(self,img_size = 32, patch_size = 2, in_chans = 3 ,embed_dim = 768):\n",
        "    super().__init__()\n",
        "    img_size = to_2tuple(img_size) # img_size = (img_size, img_size)\n",
        "    patch_size = to_2tuple(patch_size) # patch_size = (patch_size, patch_size)\n",
        "\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    # assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
        "    #     f\"img_size {img_size} should be divided by patch_size {patch_size}.\"\n",
        "    self.H, self.W = img_size[0] * patch_size[0], img_size[1] * patch_size[1] # H,W: number of patches in vertical and horizontal directions respectively as a 2D map -- Considering each patch as a pixel, H and W makes sense\n",
        "    # self.num_patches = self.H * self.W\n",
        "    self.proj = nn.ConvTranspose2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) # Nonoverlapping convolutions to embed each patch\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x) -> Tuple[torch.Tensor, Size_]:\n",
        "    B, C, H, W = x.shape # Apparently first input is an image with dimensions: B, C, orig_H, orig_W\n",
        "\n",
        "    x = self.proj(x).flatten(2).transpose(1, 2) # x: B, H'*W', embed_dim -- H' and W' are the new image where every pixel is actually a patch with 768 dimensions\n",
        "    x = self.norm(x)\n",
        "    out_size = (H * self.patch_size[0], W * self.patch_size[1])\n",
        "\n",
        "    return x, out_size # Return both the patch embedded x and its new size\n",
        "\n",
        "class Inv_Twins(nn.Module):\n",
        "    \"\"\" Twins Vision Transfomer (Revisiting Spatial Attention)\n",
        "    Adapted from PVT (PyramidVisionTransformer) class at https://github.com/whai362/PVT.git\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self, img_size=2, patch_size=2, in_chans=512, num_classes=1000, global_pool='avg',\n",
        "            embed_dims=(512, 256, 128, 64), num_heads=(1,2,4,8), mlp_ratios=(4, 4, 4, 4), depths=(3, 4, 6, 3),\n",
        "            sr_ratios=(1,2,4,8), wss=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
        "            norm_layer=partial(nn.LayerNorm, eps=1e-6), block_cls=Block):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes # For image classification\n",
        "        self.global_pool = global_pool\n",
        "        self.depths = depths\n",
        "        self.embed_dims = embed_dims\n",
        "        self.num_features = embed_dims[-1]\n",
        "        self.grad_checkpointing = False\n",
        "\n",
        "        img_size = to_2tuple(img_size)\n",
        "        prev_chs = in_chans # original image channels\n",
        "        self.patch_embeds = nn.ModuleList()\n",
        "        self.pos_drops = nn.ModuleList() # For dropout\n",
        "        for i in range(len(depths)):\n",
        "            self.patch_embeds.append(Inv_PatchEmbed(img_size, patch_size, prev_chs, embed_dims[i])) # Initilialize the PatchEmbed modules with different embed_dim for each layer\n",
        "            self.pos_drops.append(nn.Dropout(p=drop_rate))\n",
        "            prev_chs = embed_dims[i] # Update the prev_chs so we can use PatchEmbed modules\n",
        "            img_size = tuple(t * patch_size for t in img_size) # Gradual patching, declaring the new img sizes while passing through each layer\n",
        "            patch_size = 2\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "        cur = 0\n",
        "        for k in range(len(depths)):\n",
        "            _block = nn.ModuleList([block_cls(\n",
        "                dim=embed_dims[k], num_heads=num_heads[k], mlp_ratio=mlp_ratios[k], drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[k],\n",
        "                ws=1 if wss is None or i % 2 == 1 else wss[k]) for i in range(depths[k])]) # In a layer with depth 3 for example the i=0 th layer will do LSA and i = 1 will do GloalSubsampled\n",
        "            self.blocks.append(_block)\n",
        "            cur += depths[k]\n",
        "\n",
        "        self.pos_block = nn.ModuleList([Inv_PosConv(embed_dim, embed_dim) for embed_dim in embed_dims])\n",
        "\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "        self.proj = nn.Linear(embed_dims[-1],3)\n",
        "        self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0] # x: B,H,W,C\n",
        "        for i, (inv_embed, drop, blocks, pos_blk) in enumerate(\n",
        "                zip(self.patch_embeds, self.pos_drops, self.blocks, self.pos_block)):\n",
        "            x, size = inv_embed(x) # First create and embed the patches -- B,H*2,W*2, embed_dim[0] ->\n",
        "            # print(size)\n",
        "            x = drop(x)\n",
        "            for j, blk in enumerate(blocks): # Pass thorugh attention modules but after the first layer add PEG \n",
        "                x = blk(x, size)  # x: B,H*2,W*2,embed_dim[0]\n",
        "                if j == 0:\n",
        "                    x = pos_blk(x, size)  # x: B,H*2,W*2,embed_dim[0]\n",
        "            if i < len(self.depths) - 1: # Reshape the x to B, C, H, W unless it is the final output tensor, the final x is x: B, last_H*last_W, last_embedding_dim\n",
        "                x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        x = self.norm(x)\n",
        "        x = x.reshape(B, *size, -1).contiguous()\n",
        "        x = self.proj(x)\n",
        "        x = self.act(x)\n",
        "        x = x.reshape(B,-1,*size)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "3iWlBGVZidct"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MergedAutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MergedAutoEncoder,self).__init__()\n",
        "        ########## encoder #########\n",
        "        self.encoder = Twins(img_size = 32, patch_size = 2, in_chans = 3, embed_dims = (64,128,256,512), num_heads = (1,2,4,8), mlp_ratios = (4,4,4,4),\n",
        "                            depths = (3,4,6,3),sr_ratios = (8,4,2,1), wss = (2,2,2,2))\n",
        "        self.decoder = Inv_Twins(img_size = 2, patch_size = 2,in_chans =512,embed_dims=(512, 256, 128, 64), num_heads=(1,2,4,8), mlp_ratios=(4, 4, 4, 4), depths=(3, 4, 6, 3), sr_ratios=(1,2,4,8), wss=(2,2,2,2))\n",
        "    \n",
        "    def encode(self,x):\n",
        "            x = self.encoder(x)\n",
        "            return x\n",
        "    \n",
        "    def decode(self,x):\n",
        "      x = self.decoder(x)\n",
        "      return x\n",
        "        \n",
        "    def forward(self,x):\n",
        "            x = self.encode(x)\n",
        "            x = self.decode(x)\n",
        "            return x\n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T15:00:14.280557Z",
          "iopub.execute_input": "2022-08-05T15:00:14.280958Z",
          "iopub.status.idle": "2022-08-05T15:00:14.288136Z",
          "shell.execute_reply.started": "2022-08-05T15:00:14.280925Z",
          "shell.execute_reply": "2022-08-05T15:00:14.287161Z"
        },
        "trusted": true,
        "id": "dc5by9JPh6s1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs= torch.rand(5,3,32,32)\n",
        "autoencoder = MergedAutoEncoder()\n",
        "encoder_out = autoencoder.encode(imgs)\n",
        "print(encoder_out.shape)\n",
        "x = autoencoder(imgs)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T15:00:15.664974Z",
          "iopub.execute_input": "2022-08-05T15:00:15.665719Z",
          "iopub.status.idle": "2022-08-05T15:00:15.844639Z",
          "shell.execute_reply.started": "2022-08-05T15:00:15.665684Z",
          "shell.execute_reply": "2022-08-05T15:00:15.843276Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKnaPEe7h6s2",
        "outputId": "a115c7f6-8d57-4bf9-c3f9-5ffa18cf9e96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 512, 2, 2])\n",
            "torch.Size([5, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import loss\n",
        "from torchvision.transforms.transforms import RandomVerticalFlip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "      # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    #  transforms.RandomCrop(32,padding=4),\n",
        "    #  transforms.RandomVerticalFlip(p=0.5),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "    ])# normalize the image between [-1 1]\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "    ])\n",
        "\n",
        "batch_size = 128\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "model = MergedAutoEncoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='max',patience=2,verbose=True,factor = 0.75)\n",
        "\n",
        "def compute_psnr(img1, img2):\n",
        "    img1 = img1.astype(np.float64) \n",
        "    img2 = img2.astype(np.float64) \n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return \"Same Image\"\n",
        "    return 10 * math.log10(1. / mse)\n",
        "\n",
        "######## Training #########\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X = X.to(device) \n",
        "    pred = model(X)\n",
        "    loss = criterion(pred,X) # The difference between X and  the prediction by model\n",
        "    with torch.autograd.set_detect_anomaly(False):\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch % 100 == 0:\n",
        "        loss,current = loss.item(), batch * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader,model,loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0\n",
        "  psnr = 0\n",
        "  with torch.no_grad():\n",
        "    for X,y in dataloader:\n",
        "      X = X.to(device)\n",
        "      pred = model(X)\n",
        "      psnr += compute_psnr(pred.cpu().numpy(),X.cpu().numpy())\n",
        "      test_loss += criterion(pred,X).item()\n",
        "    print(f\"PSNR: {psnr/num_batches}\")\n",
        "    print(f\"Test Loss: {test_loss/num_batches}\")\n",
        "  return psnr/num_batches\n",
        "\n",
        "epochs = 60\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, criterion, optimizer)\n",
        "    PSNR = test(testloader, model, criterion)\n",
        "    scheduler.step(PSNR)\n",
        "    # print(f\"The last LR is {scheduler.get_last_lr()[0]}\")\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "kiM5JxfHiSL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef19189d-e66a-4ad5-8789-f07039773375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.478039  [    0/50000]\n",
            "loss: 0.097693  [12800/50000]\n",
            "loss: 0.055825  [25600/50000]\n",
            "loss: 0.050436  [38400/50000]\n",
            "PSNR: 13.693213176065301\n",
            "Test Loss: 0.042770322270785706\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.042470  [    0/50000]\n",
            "loss: 0.039563  [12800/50000]\n",
            "loss: 0.034402  [25600/50000]\n",
            "loss: 0.030132  [38400/50000]\n",
            "PSNR: 15.254587642512448\n",
            "Test Loss: 0.029852681687172457\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.029184  [    0/50000]\n",
            "loss: 0.028585  [12800/50000]\n",
            "loss: 0.027231  [25600/50000]\n",
            "loss: 0.025024  [38400/50000]\n",
            "PSNR: 16.236091734774458\n",
            "Test Loss: 0.023813980101030086\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.022201  [    0/50000]\n",
            "loss: 0.022525  [12800/50000]\n",
            "loss: 0.020555  [25600/50000]\n",
            "loss: 0.020343  [38400/50000]\n",
            "PSNR: 16.969306491549947\n",
            "Test Loss: 0.020113866163205495\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.019214  [    0/50000]\n",
            "loss: 0.021647  [12800/50000]\n",
            "loss: 0.018343  [25600/50000]\n",
            "loss: 0.018291  [38400/50000]\n",
            "PSNR: 17.522027653975314\n",
            "Test Loss: 0.01771019986252996\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.017691  [    0/50000]\n",
            "loss: 0.018404  [12800/50000]\n",
            "loss: 0.015772  [25600/50000]\n",
            "loss: 0.016670  [38400/50000]\n",
            "PSNR: 18.03616884213525\n",
            "Test Loss: 0.015733263797209233\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.016600  [    0/50000]\n",
            "loss: 0.016994  [12800/50000]\n",
            "loss: 0.017746  [25600/50000]\n",
            "loss: 0.014659  [38400/50000]\n",
            "PSNR: 18.408805802070496\n",
            "Test Loss: 0.0144394194424341\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.014575  [    0/50000]\n",
            "loss: 0.014524  [12800/50000]\n",
            "loss: 0.014392  [25600/50000]\n",
            "loss: 0.014526  [38400/50000]\n",
            "PSNR: 18.706635300973698\n",
            "Test Loss: 0.013482687228559694\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.013457  [    0/50000]\n",
            "loss: 0.013656  [12800/50000]\n",
            "loss: 0.012344  [25600/50000]\n",
            "loss: 0.012375  [38400/50000]\n",
            "PSNR: 19.047808755028335\n",
            "Test Loss: 0.012464461883506443\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.011152  [    0/50000]\n",
            "loss: 0.012557  [12800/50000]\n",
            "loss: 0.012365  [25600/50000]\n",
            "loss: 0.011663  [38400/50000]\n",
            "PSNR: 19.28642146538078\n",
            "Test Loss: 0.011798369450659691\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.011530  [    0/50000]\n",
            "loss: 0.011622  [12800/50000]\n",
            "loss: 0.009925  [25600/50000]\n",
            "loss: 0.011445  [38400/50000]\n",
            "PSNR: 19.35912405023381\n",
            "Test Loss: 0.011601926735307596\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.010362  [    0/50000]\n",
            "loss: 0.011130  [12800/50000]\n",
            "loss: 0.010337  [25600/50000]\n",
            "loss: 0.014415  [38400/50000]\n",
            "PSNR: 19.695573438066013\n",
            "Test Loss: 0.01073790908661447\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.009845  [    0/50000]\n",
            "loss: 0.011637  [12800/50000]\n",
            "loss: 0.011292  [25600/50000]\n",
            "loss: 0.009768  [38400/50000]\n",
            "PSNR: 19.66065125189165\n",
            "Test Loss: 0.010823341169029096\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.010618  [    0/50000]\n",
            "loss: 0.011830  [12800/50000]\n",
            "loss: 0.010327  [25600/50000]\n",
            "loss: 0.010112  [38400/50000]\n",
            "PSNR: 20.094593413678815\n",
            "Test Loss: 0.009795623100539552\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.009750  [    0/50000]\n",
            "loss: 0.009006  [12800/50000]\n",
            "loss: 0.008977  [25600/50000]\n",
            "loss: 0.008993  [38400/50000]\n",
            "PSNR: 20.266227787234328\n",
            "Test Loss: 0.00941630975119298\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.009517  [    0/50000]\n",
            "loss: 0.009015  [12800/50000]\n",
            "loss: 0.009307  [25600/50000]\n",
            "loss: 0.008526  [38400/50000]\n",
            "PSNR: 20.517564437282104\n",
            "Test Loss: 0.008886970556046389\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.008577  [    0/50000]\n",
            "loss: 0.008711  [12800/50000]\n",
            "loss: 0.007607  [25600/50000]\n",
            "loss: 0.008679  [38400/50000]\n",
            "PSNR: 20.250425403376617\n",
            "Test Loss: 0.009448711064797414\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.009468  [    0/50000]\n",
            "loss: 0.008318  [12800/50000]\n",
            "loss: 0.008451  [25600/50000]\n",
            "loss: 0.008428  [38400/50000]\n",
            "PSNR: 20.873662302069057\n",
            "Test Loss: 0.008187660988703181\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.007587  [    0/50000]\n",
            "loss: 0.007805  [12800/50000]\n",
            "loss: 0.008388  [25600/50000]\n",
            "loss: 0.009974  [38400/50000]\n",
            "PSNR: 20.981063518416743\n",
            "Test Loss: 0.007987661458127484\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.007557  [    0/50000]\n",
            "loss: 0.007819  [12800/50000]\n",
            "loss: 0.008005  [25600/50000]\n",
            "loss: 0.007682  [38400/50000]\n",
            "PSNR: 21.238638603721913\n",
            "Test Loss: 0.007528101654039531\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.007163  [    0/50000]\n",
            "loss: 0.007343  [12800/50000]\n",
            "loss: 0.006462  [25600/50000]\n",
            "loss: 0.007165  [38400/50000]\n",
            "PSNR: 21.3041885302548\n",
            "Test Loss: 0.0074153731494576115\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.006928  [    0/50000]\n",
            "loss: 0.007751  [12800/50000]\n",
            "loss: 0.007507  [25600/50000]\n",
            "loss: 0.007145  [38400/50000]\n",
            "PSNR: 21.375209545358988\n",
            "Test Loss: 0.007294735865360951\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.007495  [    0/50000]\n",
            "loss: 0.006570  [12800/50000]\n",
            "loss: 0.006809  [25600/50000]\n",
            "loss: 0.007010  [38400/50000]\n",
            "PSNR: 21.66249391564531\n",
            "Test Loss: 0.0068285504969049105\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.006375  [    0/50000]\n",
            "loss: 0.007051  [12800/50000]\n",
            "loss: 0.006524  [25600/50000]\n",
            "loss: 0.006505  [38400/50000]\n",
            "PSNR: 21.637400578304014\n",
            "Test Loss: 0.0068675250027187265\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.007311  [    0/50000]\n",
            "loss: 0.006146  [12800/50000]\n",
            "loss: 0.006211  [25600/50000]\n",
            "loss: 0.006334  [38400/50000]\n",
            "PSNR: 21.94603839332454\n",
            "Test Loss: 0.006397144680347623\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.006047  [    0/50000]\n",
            "loss: 0.005951  [12800/50000]\n",
            "loss: 0.006080  [25600/50000]\n",
            "loss: 0.005708  [38400/50000]\n",
            "PSNR: 22.028226113585507\n",
            "Test Loss: 0.006277064627767364\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.006060  [    0/50000]\n",
            "loss: 0.006214  [12800/50000]\n",
            "loss: 0.005163  [25600/50000]\n",
            "loss: 0.006078  [38400/50000]\n",
            "PSNR: 22.026542918505864\n",
            "Test Loss: 0.006279074846283544\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.006720  [    0/50000]\n",
            "loss: 0.005504  [12800/50000]\n",
            "loss: 0.006064  [25600/50000]\n",
            "loss: 0.005350  [38400/50000]\n",
            "PSNR: 22.43163113704631\n",
            "Test Loss: 0.0057208248048643525\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.005765  [    0/50000]\n",
            "loss: 0.005610  [12800/50000]\n",
            "loss: 0.005495  [25600/50000]\n",
            "loss: 0.005205  [38400/50000]\n",
            "PSNR: 22.511382516068355\n",
            "Test Loss: 0.00561679113373349\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.005713  [    0/50000]\n",
            "loss: 0.005746  [12800/50000]\n",
            "loss: 0.005379  [25600/50000]\n",
            "loss: 0.005493  [38400/50000]\n",
            "PSNR: 22.62308859772337\n",
            "Test Loss: 0.005474186337352554\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.006066  [    0/50000]\n",
            "loss: 0.004975  [12800/50000]\n",
            "loss: 0.005218  [25600/50000]\n",
            "loss: 0.005140  [38400/50000]\n",
            "PSNR: 22.62794152082766\n",
            "Test Loss: 0.005467755289724733\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.005213  [    0/50000]\n",
            "loss: 0.005516  [12800/50000]\n",
            "loss: 0.004841  [25600/50000]\n",
            "loss: 0.005295  [38400/50000]\n",
            "PSNR: 22.799142100534596\n",
            "Test Loss: 0.005256837664195631\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.005007  [    0/50000]\n",
            "loss: 0.004494  [12800/50000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model\n",
        "torch.save(model.state_dict(), \"model-Twins_Autoencoder.pth\")\n",
        "print(\"Saved PyTorch Model State to model-Twins_Autoencoder.pth\")\n",
        "from google.colab import files\n",
        "files.download( \"model-Twins_Autoencoder.pth\" ) "
      ],
      "metadata": {
        "id": "i9aE5EDvD2T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nPBdGhM4DqQg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}